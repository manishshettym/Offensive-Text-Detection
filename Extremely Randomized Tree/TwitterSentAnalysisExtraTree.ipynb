{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Hate Speech Sentiment Analysis\n",
    "This code was written in order to solve the challenge on this <a href=\"https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/\">link</a>.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries for dataset preparation, feature engineering, model training\n",
    "from sklearn import model_selection, preprocessing, metrics, svm, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd, numpy, string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "#Remove Special Charactors\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation and Cleaning\n",
    "\n",
    "For the purpose of this exercise, I have removed the special characters, converted words to lower case and made sure there are no null rows. Also I have created a data frame with two columns, tweets and labels to represent the excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (31962, 3) 31962\n",
      "Test Set: (17197, 2) 17197\n"
     ]
    }
   ],
   "source": [
    "porter=PorterStemmer()\n",
    "#Import Training and Testing Data\n",
    "train = pd.read_csv('train.csv')\n",
    "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
    "test = pd.read_csv('test_tweets.csv')\n",
    "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
    "#Tokenize words in order to clean and stem\n",
    "tok = WordPunctTokenizer()\n",
    "# patterns to remove html tags numbers and special Characters\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    #Stemming\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    #Rejoin the words back to create the cleaned tweet\n",
    "    words=\"\".join(stem_sentence).strip()    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0,len(train)]\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
    "nums = [0,len(test)]\n",
    "test_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
    "train_clean['label'] = train.label\n",
    "train_clean['id'] = train.id\n",
    "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
    "test_clean['id'] = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.<br>\n",
    "<u>Word Level TF-IDF</u> : Matrix representing tf-idf scores of every term in different documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english', max_features=100000)\n",
    "tfidf_vect.fit(train_clean['tweet'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the f1 Score\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)    \n",
    "\n",
    "    return metrics.f1_score(valid_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN Ensemble Model\n",
    "\n",
    "Extremely Randomized Trees are a type of ensemble models, particularly bagging models. They are part of the tree based model family. Extremely randomized trees pick a node split very extremely (both a variable index and variable splitting value are chosen randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely Randomized Trees, WordLevel TF-IDF:  0.6965517241379311\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.ExtraTreesClassifier(n_estimators=300), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"Extremely Randomized Trees, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return Predictions Label\n",
    "def train_model_Pred(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now working with Real challenge Data\n",
    "train_x=train_clean['tweet']\n",
    "valid_x=test_clean['tweet']\n",
    "train_y=train_clean['label']\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english', max_features=100000)\n",
    "tfidf_vect.fit(train_clean['tweet'])\n",
    "tfidf_vect.fit(test_clean['tweet'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely Randomized Trees, WordLevel TF-IDF:  [0 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees on Word Level TF IDF Vectors\n",
    "accuracy = train_model_Pred(ensemble.ExtraTreesClassifier(n_estimators=300), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"Extremely Randomized Trees, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to DF and then export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'id':test['id'],'Tweet':valid_x,'label':accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
